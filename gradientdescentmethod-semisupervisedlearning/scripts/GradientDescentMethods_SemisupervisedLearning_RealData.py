# -*- coding: utf-8 -*-
"""GradientDescentMethods_SemisupervisedLearning_RealData .ipynb

Automatically generated by Colab.

# Try on Datasets

##Breast Cancer Dataset (Winsconsin)

Breast Cancer Dataset Gradient Descent
"""

np.random.seed(77)

#Trying Cancer Dataset with GD

from sklearn.datasets import load_breast_cancer
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist, squareform, pdist
import time

cancer = load_breast_cancer()

X=cancer.data
y=cancer.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(
    X, y, test_size=0.8, stratify=y
)

L = len(X_labeled)
U = len(X_unlabeled)

labeled_coords = X_labeled[:, :]
unlabeled_coords = X_unlabeled[:, :]

#Matrices
W = cdist(labeled_coords, unlabeled_coords, metric='euclidean')
W_hat= squareform(pdist(unlabeled_coords, metric='euclidean'))
epsilon = 1e-8

W = 1 / (W + epsilon)
W_hat = 1 / (W_hat + epsilon)




# Initial guess for unlabeled outputs
y_unlabeled = np.random.rand(U)
y_labeled = y_labeled.astype(float)

# === STEP 2: Define the loss function and its gradient ===
def loss_function(W, W_hat, labeled_labels, y_unlabeled):
    term1 = 0
    for i in range(L):
        for j in range(U):
            diff = y_unlabeled[j] - labeled_labels[i]
            term1 += W[i, j] * (diff ** 2)

    term2 = 0
    for i in range(U):
        for j in range(U):
            diff = y_unlabeled[i] - y_unlabeled[j]
            term2 += W_hat[i, j] * (diff ** 2)

    term2 *= 0.5
    return term1 + term2

def gradient_yj(j, W, W_hat, labeled_labels, y_unlabeled):
    term1 = 0
    for i in range(L):
        term1 += W[i, j] * (y_unlabeled[j] - labeled_labels[i])

    term2 = 0
    for i in range(U):
        term2 += W_hat[i, j] * (y_unlabeled[j] - y_unlabeled[i])

    return 2 * term1 + term2


# === STEP 3: Gradient Descent Algorithm ===
alpha = 0.001
max_iter = 500
tolerance = 0.1

loss_history = []

start_time = time.process_time()

for iteration in range(max_iter):
    current_loss = loss_function(W, W_hat, y_labeled, y_unlabeled)
    loss_history.append(current_loss)

    gradients = np.zeros(U)
    for j in range(U):
        gradients[j] = gradient_yj(j, W, W_hat, y_labeled, y_unlabeled)

    gradients = np.clip(gradients, -10, 10)
    y_unlabeled -= alpha * gradients

    grad_norm = np.linalg.norm(gradients)
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}")
        break

    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")

end_time = time.process_time()
cpu_time = end_time - start_time

# === STEP 4: Evaluate accuracy ===
y_pred = (y_unlabeled > 0.7).astype(int)
acc = accuracy_score(y_unlabeled_true, y_pred)
print(f"\nAccuracy on unlabeled data: {acc:.4f}")
print(f"CPU time for gradient descent: {cpu_time:.2f} seconds")
print(y_pred)
print(y_unlabeled_true)


# === STEP 5: Plot loss over time ===
plt.plot(loss_history)
plt.title("Loss Over Iterations (Gradient Descent)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""Breast Cancer Dataset, Gradient Descent with Armijo Rule"""

np.random.seed(77)

#Armijo Rule GD

from sklearn.datasets import load_breast_cancer
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist, squareform, pdist

cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(
    X, y, test_size=0.8, stratify=y
)

L = len(X_labeled)
U = len(X_unlabeled)

labeled_coords = X_labeled[:, :]
unlabeled_coords = X_unlabeled[:, :]

#Matrices
W = cdist(labeled_coords, unlabeled_coords, metric='euclidean')
W_hat= squareform(pdist(unlabeled_coords, metric='euclidean'))
epsilon = 1e-8
W = 1 / (W + epsilon)
W_hat = 1 / (W_hat + epsilon)


# Initial guess for unlabeled outputs
y_unlabeled = np.random.rand(U)
y_labeled = y_labeled.astype(float)

max_iter = 500  # Number of iterations
alpha_init = 1.0  # Initial step size

delta = 0.5  # Shrinkage base (0 < delta < 1)
gamma = 0.05  # Sufficient decrease parameter (0 < gamma < 0.5)

# Stopping conditions
max_m = 10
tolerance = 0.1
max_m_reached = False

# Placeholder for the loss history to track convergence
loss_history = []
start_time = time.process_time()

for iteration in range(max_iter):
    current_loss = loss_function(W, W_hat, y_labeled, y_unlabeled)  # Use y_labeled here
    loss_history.append(current_loss)  # Store the loss

    gradients = np.zeros(U)
    for j in range(U):
        gradients[j] = gradient_yj(j, W, W_hat, y_labeled, y_unlabeled)  # Use y_labeled here

    d = -gradients  # Descent directions
    dot_product = np.dot(gradients, d)

    # Armijo rule for step size selection
    m = 0
    while True:
        alpha = (delta ** m) * alpha_init  # Set the step size
        #print(alpha)
        #print(m)
        y_trial = y_unlabeled + alpha * d  # Compute the new labels
        loss_trial = loss_function(W, W_hat, y_labeled, y_trial)  # Use y_labeled here

        if loss_trial <= current_loss + gamma * alpha * dot_product:  # Check Armijo condition
            break
        m += 1  # Reduce the step size
        if m > max_m:
            print("Stopping: exceeded max_m in Armijo rule")
            print(f"Converged at iteration {iteration}, Loss: {current_loss:.4f}")
            max_m_reached = True
            break

    # Update y_unlabeled
    y_unlabeled += alpha * d
    grad_norm = np.linalg.norm(gradients)

    # Check for convergence
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")
        break

    if m>max_m:
      break
    if 'max_m_reached' in locals() and max_m_reached:
      print("stopping condition reached")
      break

    # Logging
    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}, Step Size: {alpha:.5f}, m: {m}")

end_time = time.process_time()  # Record end time
cpu_time = end_time - start_time

# Make predictions on the unlabeled data
y_pred = (y_unlabeled > 0.7).astype(int)

# Evaluate accuracy on the true unlabeled labels
accuracy = accuracy_score(y_unlabeled_true, y_pred)
print(f"\nAccuracy on unlabeled data: {accuracy:.4f}")
print(f"CPU time for gradient descent with Armijo rule: {cpu_time:.2f} seconds")
print(y_pred)
print(y_unlabeled_true)

# Plot loss curve
plt.plot(loss_history)
plt.title("Loss Over Iterations (Gradient Descent with Armijo Rule)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""Breast Cancer Dataset with BCDG with Gauss-Southwell Rule"""

np.random.seed(77)

#BCGD with Gauss-Southwell Rule

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score
from scipy.spatial.distance import cdist, squareform, pdist


# Load and preprocess data (as in your original code)
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(X, y, test_size=0.8, stratify=y)
L = len(X_labeled)
U = len(X_unlabeled)
labeled_coords = X_labeled[:, :]
unlabeled_coords = X_unlabeled[:, :]


#epsilon = 1e-8
#W = 1 / (cdist(labeled_coords, unlabeled_coords) + epsilon)
#W_hat= squareform(pdist(unlabeled_coords, metric='euclidean'))

W = cdist(labeled_coords, unlabeled_coords, metric='euclidean')
W_hat= squareform(pdist(unlabeled_coords, metric='euclidean'))
epsilon = 1e-8
W = 1 / (W + epsilon)
W_hat = 1 / (W_hat + epsilon)


# Initial values
y_unlabeled = np.random.rand(U)
y_labeled = y_labeled.astype(float)


# Block settings
block_size = 1
num_blocks = int(np.ceil(U / block_size))
blocks = [np.arange(i * block_size, min((i + 1) * block_size, U)) for i in range(num_blocks)]
  # Optimization loop
max_iter = 1000
step_size = 0.01
tolerance = 0.1
loss_history = []
start_time = time.process_time()

for iteration in range(max_iter):
    current_loss = loss_function(W, W_hat, y_labeled, y_unlabeled)
    loss_history.append(current_loss)

    #gradients = compute_gradients(W, W_hat, y_labeled, y_unlabeled)
    gradients = np.zeros(U)
    for j in range(U):
        gradients[j] = gradient_yj(j, W, W_hat, y_labeled, y_unlabeled)

      # Gauss-Southwell: pick block with largest gradient norm
    block_norms = [np.linalg.norm(gradients[block]) for block in blocks]
    block_idx = np.argmax(block_norms)
    block_to_update = blocks[block_idx]

      # Update all variables in selected block
    y_unlabeled[block_to_update] -= step_size * gradients[block_to_update]

    grad_norm = np.linalg.norm(gradients)
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")
        break

    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}, Block: {block_idx}")

end_time = time.process_time()
cpu_time = end_time - start_time
# Evaluation
#print(y_unlabeled)
y_pred = (y_unlabeled > 0.7).astype(int)

accuracy = accuracy_score(y_unlabeled_true, y_pred)
print(f"\nAccuracy on unlabeled data: {accuracy:.4f}")
print(f"CPU time for BCDG with Gauss-Southwell Rule: {cpu_time:.2f} seconds")
print(y_pred)
print(y_unlabeled_true)
# Plot
plt.plot(loss_history)
plt.title("Loss Over Iterations (Block Coordinate Descent)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

np.random.seed(77)

#Coordinate Minimization GD (sequantial order Gauss-Seidel)

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist, squareform, pdist
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score

# --- Data Loading and Preprocessing ---
cancer = load_breast_cancer()

X = cancer.data
y = cancer.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

# Use a fixed random state for reproducibility
X_labeled, X_unlabeled, y_labeled, y_unlabeled_true = train_test_split(
    X, y, test_size=0.8, stratify=y
)

L = len(X_labeled)
U = len(X_unlabeled)

labeled_coords = X_labeled[:, :]
unlabeled_coords = X_unlabeled[:, :]

# --- Weight Calculation (W and W_hat remain as in your original code) ---
W = cdist(labeled_coords, unlabeled_coords, metric='euclidean')
W_hat = squareform(pdist(unlabeled_coords, metric='euclidean'))
epsilon = 1e-8
W = 1 / (W + epsilon)
W_hat = 1 / (W_hat + epsilon)

# --- Optimization (Coordinate Minimization with modified update) ---
y_unlabeled = np.random.rand(U)
y_labeled = y_labeled.astype(float) # Ensure labeled data is float

max_iter = 500 # Maximum number of outer iterations (cycles through all coordinates)
tolerance = 1e-5 # Stopping condition: stop if change in y_unlabeled is small

loss_history = []
y_unlabeled_history = [y_unlabeled.copy()] # Track y_unlabeled for convergence check
start_time = time.process_time()
print("Starting Coordinate Minimization (Alternative Update)...")

for iteration in range(max_iter):
    y_unlabeled_old = y_unlabeled.copy()

    # Compute current loss before the cycle of updates
    # The loss calculation is correct even with the large W_hat diagonal
    current_loss = loss_function(W, W_hat, y_labeled, y_unlabeled)
    loss_history.append(current_loss)

    # --- Cycle through each unlabeled coordinate (y_unlabeled[j]) ---
    for j in range(U):
        sum_W_hat_ju_except_j = np.sum(W_hat[j, :] * y_unlabeled) - W_hat[j, j] * y_unlabeled[j]
        sum_W_hat_j_except_j = np.sum(W_hat[j, :]) - W_hat[j, j]

        numerator = 2*np.sum(W[:, j] * y_labeled) + sum_W_hat_ju_except_j
        denominator = 2*np.sum(W[:, j]) + sum_W_hat_j_except_j

        if denominator > epsilon:
             y_unlabeled[j] = numerator / denominator

    # Calculate the maximum absolute change in y_unlabeled across this iteration's cycle
    max_change = np.max(np.abs(y_unlabeled - y_unlabeled_old))
    y_unlabeled_history.append(y_unlabeled.copy()) # Store updated y_unlabeled

    if max_change < tolerance:
        print(f"Converged at iteration {iteration} with max change {max_change:.6f}")
        break

    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Max Change: {max_change:.6f}")

if iteration == max_iter - 1:
    print(f"Maximum iterations ({max_iter}) reached.")

end_time = time.process_time()
cpu_time = end_time - start_time
print("\nOptimization finished.")
print("Final y_unlabeled values:")


y_pred = (y_unlabeled > 0.7).astype(int)
print(y_unlabeled)
# Evaluate accuracy on the true unlabeled labels
accuracy = accuracy_score(y_unlabeled_true, y_pred)
print(f"\nAccuracy on unlabeled data: {accuracy:.4f}")
print(f"CPU time for Coordinate Minimization (Alternative Update): {cpu_time:.2f} seconds")
print("Predicted labels (first 20):", y_pred[:20])
print("True unlabeled labels (first 20):", y_unlabeled_true[:20])


# --- Plotting ---
# Plot the loss over iterations
plt.figure(figsize=(10, 5))
plt.plot(loss_history)
plt.title("Loss Over Iterations (Coordinate Minimization - Alternative Update)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()
