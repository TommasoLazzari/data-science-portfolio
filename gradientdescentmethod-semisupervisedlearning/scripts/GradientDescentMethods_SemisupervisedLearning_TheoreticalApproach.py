# -*- coding: utf-8 -*-
"""GradientDescentMethods_SemisupervisedLearning_TheoreticalApproach.ipynb

Automatically generated by Colab.

# **DATA GENERATION**
"""

U = 280 #number of unlabeled data points
L = 120 #number of labeled data points

import numpy as np

#random seed for reproducibility
np.random.seed(77)

#generation of random points uniformly in [0,1]^2
points = np.random.rand(U + L, 2)

import matplotlib.pyplot as plt
#plot the generated data points
plt.scatter(points[:, 0], points[:, 1], color='blue', s=10)
plt.title('400 Random Points in [0,1]^2 ')
plt.xlabel('x')
plt.ylabel('y')
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.gca().set_aspect('equal', adjustable='box')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""The following code is aimed to label the 30% of the data and to visualize it"""

#split the dataset
labeled_points = points[:L]
unlabeled_points = points[L:]
#label a portion of the data points

#generate random binary labels
labels = np.random.randint(0, 2, size=(L, 1))
#and append them to the now labeled dataset
labeled_points = np.hstack((labeled_points, labels))

#plot
plt.scatter(unlabeled_points[:, 0], unlabeled_points[:, 1], color='gray', s=10, label='Unlabeled')
plt.scatter(labeled_points[labeled_points[:, 2] == 0][:, 0], labeled_points[labeled_points[:, 2] == 0][:, 1],
            color='blue', s=10, label='Labeled: Class 0')
plt.scatter(labeled_points[labeled_points[:, 2] == 1][:, 0], labeled_points[labeled_points[:, 2] == 1][:, 1],
            color='red', s=10, label='Labeled: Class 1')
plt.title('Labeled Points with Binary Labels vs Unlabeled Points')
plt.xlabel('x')
plt.ylabel('y')
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.gca().set_aspect('equal', adjustable='box')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""# **WEIGHT COMPUTATION**"""

#WEIGHTS W (similarity measure between labeled and unlabeled data points)

from scipy.spatial.distance import cdist

#compute the distance matrix between labeled (rows) and unlabeled (columns) data
#each element w_ij of the matrix will be the euclidean distance between the i-th labeled data point and j-th unlabeled data point
labeled_coords = labeled_points[:, :2]
W = cdist(labeled_coords, unlabeled_points, metric='euclidean')
W = W / W.max()
W = 1 - W


print("Shape of W:", W.shape)
print("Sample of W:\n", W[:5, :5])  #just to inspect the top-left corner

from scipy.spatial.distance import squareform, pdist

#efficient way to compute an intra-set distance, since distance from A to B, will be the same as from B to A
W_hat = squareform(pdist(unlabeled_points, metric='euclidean'))
W_hat = W_hat / W_hat.max()
W_hat = 1 - W_hat

print("Shape of W_hat:", W_hat.shape)
print("Sample of W_hat:\n", W_hat[:5, :5])

"""# **LOSS FUNCTION**"""

#LOSS FUNCTION

def loss_function(W, W_hat, labeled_labels, y_unlabeled):

    #first term: weighted sum of the differences between labeled and unlabeled data points
    term1 = 0
    for i in range(L):
        for j in range(U):
            diff = y_unlabeled[j] - labeled_labels[i]
            term1 += W[i, j] * (diff ** 2)

    #second term: half weighted sum of the differences between unlabeled data points
    term2 = 0
    for i in range(U):
        for j in range(U):
            diff = y_unlabeled[i] - y_unlabeled[j]
            term2 += W_hat[i, j] * (diff ** 2)

    term2 *= 0.5

    return term1 + term2

#FIRST DERIVATE OF THE LOSS FUNCTION

def gradient_yj(j, W, W_hat, labeled_labels, y_unlabeled):

    #first term first derivate with respect to y_j
    term1 = 0
    for i in range(L):
        term1 += W[i, j] * (y_unlabeled[j] - labeled_labels[i])

    #second term first derivate with respect to y_j
    term2 = 0
    for i in range(U):
        term2 += W_hat[i, j] * (y_unlabeled[j] - y_unlabeled[i])

    #eturn 2*term1 + term2
    return  2*term1 + 2*term2

"""# **GRADIENT DESCENT**

Gradient Descent with fixed stepsize
"""

#(Arbitrary) FIXED STEP SIZE
np.random.seed(77)

# Hyperparameters
alpha = 0.001   # Learning rate (arbitrarily set)
max_iter = 500  # Number of iterations
tolerance = 1e-4  # Stopping criterion

# Initialize y_unlabeled with random values
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points



# Placeholder for the loss history to track convergence
loss_history = []

# Gradient Descent Loop
for iteration in range(max_iter):
    # Compute current loss
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
    loss_history.append(current_loss)
    # Create empty gradient array (one value per unlabeled point)
    gradients = np.zeros(U)
    # Compute gradients (using the gradient_yj function)
    gradients = np.clip(gradients, -10, 10)  # Clip gradients to a max value

    for j in range(U):
       gradients[j] = gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled)

    # Update y_unlabeled using gradient descent update rule
    y_unlabeled -= alpha * gradients

    # Check for convergence (based on the gradient norm)
    grad_norm = np.linalg.norm(gradients)
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}")
        break

    # Print progress every 50 iterations
    if iteration % 50 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")


plt.plot(loss_history)
plt.title("Loss Over Iterations (Gradient Descent with Fixed StepSize)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# Apply thresholding to convert continuous predictions to binary labels
threshold = 0.45
y_unlabeled = np.random.rand(U)  # Predicted values for the unlabeled points (continuous)
predicted_labels = (y_unlabeled >= threshold).astype(int)

# Combine the labeled points and the newly predicted labels for the unlabeled points
final_labeled_points = np.vstack([labeled_points, np.hstack([unlabeled_points, predicted_labels.reshape(-1, 1)])])

# Plot the unlabeled points with their newly assigned predicted labels (0 or 1)
plt.scatter(unlabeled_points[predicted_labels == 0][:, 0], unlabeled_points[predicted_labels == 0][:, 1],
            color='green', s=10, label='Predicted: Class 0')
plt.scatter(unlabeled_points[predicted_labels == 1][:, 0], unlabeled_points[predicted_labels == 1][:, 1],
            color='pink', s=10, label='Predicted: Class 1')

# Also plot the original labeled points for reference
plt.scatter(labeled_points[labeled_points[:, 2] == 0][:, 0], labeled_points[labeled_points[:, 2] == 0][:, 1],
            color='blue', s=10, label='Labeled: Class 0')
plt.scatter(labeled_points[labeled_points[:, 2] == 1][:, 0], labeled_points[labeled_points[:, 2] == 1][:, 1],
            color='red', s=10, label='Labeled: Class 1')

# Adding plot title and labels
plt.title('Labeled vs Predicted Labels for Unlabeled Points')
plt.xlabel('x')
plt.ylabel('y')
plt.xlim(0, 1)
plt.ylim(0, 1)
plt.gca().set_aspect('equal', adjustable='box')

# Adding legend to differentiate between classes
plt.legend()

# Show the grid and plot
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""Estimation of Lipschitz Constant"""

#LIPSCHITZ CONSTANT
np.random.seed(77)

#Since our loss function is quadratic and its first derivate is linear (e.g. DL(y) = Ay + b),
#the rate at which the gradient changes is just determined by the matrix multiplying y (A).
#So, how fast does the gradient change? It depends on the largest eigenvalue of A,
#because eigenvalues tell how much the matrix can 'stretch' a vector
        #and in gradient descent we ant to avoid
        #taking steps that overshoot the minimum.
        #If the function has directions where it changes very quickly (big eigenvalues)
        #we need to take smaller steps in those directions
        #(and therefore we use the inverse of Lipschitz constant)
#So, our loss functions has a Lipschitz constant equal to the largest eigenvalue of the matrix multiplying y (A),

#that, in our case, is W^T W + L_hat, with
    #W the already computed weights similarity matrix (between labeled and unlabeled data)
#L_hat = D_hat - W_hat, where
    #W_hat is the already computed similarity matrix (between unlabeled data)
    #D_hat is a diagonal matrix in which each entry is the row sum of W_hat
#this term appears in the second term of the loss function, that, in fact can also be written as y^T L_hat y,
#with the goal of penalizing differences between connected nodes, enforcing smoothness.

#Putting everything together, the loss function can also be written as
  #L(y) = y^T (W^T W + L_hat) y + (linear terms)
  #DL(y) = 2(W^T W + L_hat) y + (constant vector)
#And, therefore:
  #Lipschitz constant = 2 * lambda_max(W^T W + L_hat)

#Graph Laplacian
D_hat = np.diag(W_hat.sum(axis=1))
L_hat = D_hat - W_hat

#Approximation of Lipschitz constant
W_transpose_W = np.dot(W.T, W)
H = 2 * (W_transpose_W + L_hat)
eigvals = np.linalg.eigvalsh(H)
L_lipschitz = np.max(eigvals)

# Use 1 / L as learning rate
alpha_lipschitz = 1.0 / L_lipschitz
print(f"Estimated Lipschitz constant: {L_lipschitz:.4f}, Step size (1/L): {alpha_lipschitz:.6f}")

#(1 / LipschitzConstant) FIXED STEP SIZE
np.random.seed(77)

max_iter = 1500  # Number of iterations
## Stopping criterions
tolerance = 1e-4
min_improvement = 1e-2

# Initialize y_unlabeled with random values
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points

# Placeholder for the loss history to track convergence
loss_history = []

# Gradient Descent Loop
for iteration in range(max_iter):
    # Compute current loss
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
    loss_history.append(current_loss)
    # Create empty gradient array (one value per unlabeled point)
    gradients = np.zeros(U)
    # Compute gradients (using the gradient_yj function)
    gradients = np.clip(gradients, -10, 10)  # Clip gradients to a max value
    for j in range(U):
        gradients[j] = gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled)

    # Update y_unlabeled using gradient descent update rule
    y_unlabeled -= alpha_lipschitz * gradients

    # Check for convergence (based on the gradient norm)
    grad_norm = np.linalg.norm(gradients)
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}")
        break

    if iteration >3 and (loss_history[-3] - loss_history[-1]) < min_improvement:
        print(f"Converged at iteration {iteration}")
        break


    # Print progress every 50 iterations
    if iteration % 50 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")

plt.plot(loss_history)
plt.title("Loss Over Iterations (Gradient Descent with Fixed StepSize)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""Armijo Rule Hyperparameters fine tuning"""

#ARMIJO RULE HYPERPARAMETERS FINE TUNING

import itertools

max_iter = 500
alpha_init = 1.0

#grid of values
delta_values = [0.3, 0.5, 0.7]   # 0 < delta < 1
gamma_values = [ 1e-3, 1e-2, 0.05, 0.1]  # 0 < gamma < 0.5

results = []

max_m = 50
tolerance = 1e-4

#loop over all possible combinations of delta and gamma included in the grid
for delta, gamma in itertools.product(delta_values, gamma_values):
    np.random.seed(77)

    y_unlabeled = np.random.rand(U)
    loss_history = []

    for iteration in range(max_iter):
        current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
        loss_history.append(current_loss)

        gradients = np.zeros(U)
        for j in range(U):
            gradients[j] = gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled)

        d = -gradients
        dot_product = np.dot(gradients, d)

        m = 0
        while True:
            alpha = (delta ** m) * alpha_init
            y_trial = y_unlabeled + alpha * d
            loss_trial = loss_function(W, W_hat, labels.flatten(), y_trial)

            if loss_trial <= current_loss + gamma * alpha * dot_product:
                break
            m += 1
            if m > max_m:
                break

        y_unlabeled += alpha * d
        grad_norm = np.linalg.norm(gradients)

        if grad_norm < tolerance or m > max_m:
            break

    results.append({
        'delta': delta,
        'gamma': gamma,
        'final_loss': loss_history[-1],
        'iterations': iteration,
        'converged': grad_norm < tolerance,
        'final_grad_norm': grad_norm
    })

#sort the results stored in results by their number of iterations
sorted_results = sorted(results, key=lambda x: (not x['converged'], x['iterations']))

for res in sorted_results[:5]:
    print(f"Delta: {res['delta']:.2f}, Gamma: {res['gamma']:.4f}, "
          f"Final Loss: {res['final_loss']:.4f}, Iterations: {res['iterations']}, "
          f"Converged: {res['converged']}")

"""Gradient Descent with Armijo Rule"""

#ARMIJO RULE
np.random.seed(77)

max_iter = 500           #Number of iterations
alpha_init = 1.0         #initial step size
#hyperparameters fine tuned:
delta = 0.5             #shrinkage base (0 < delta < 1)
gamma = 0.05            #sufficient decrease parameter (0 < gamma < 0.5)

#stopping conditions
max_m = 10
tolerance = 1e-4
max_m_reached = False

#y_unlabeled random values initialization
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points

# Placeholder for the loss history to track convergence
loss_history = []

for iteration in range(max_iter):
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled) #computes the loss
    loss_history.append(current_loss) #and appends it in a list where the loss function values will be stored for the whole process, to track the improvements

    gradients = np.zeros(U)
    for j in range(U):
        gradients[j] = gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled) #compute the gradient of the loss function with respect to each y_unlabeled

    d = -gradients #descent directions
    dot_product = np.dot(gradients, d)

    #now we need to decide the size of the learning rate to compute a step towards the minimum of the loss function, as per the Armijo Rule
    m = 0
    while True:
        alpha = (delta ** m) * alpha_init #set the step size
        y_trial = y_unlabeled + alpha * d #compute the new labels with the just fixed step size
        loss_trial = loss_function(W, W_hat, labels.flatten(), y_trial) #compute the loss function with the just computed labels

        if loss_trial <= current_loss + gamma * alpha * dot_product: #check the step would lead the algorithm to a point in the loss function lower enough to the previous one to justify the step
            break
        m += 1 #otherwise reduce the step size
        if m > max_m:
            print("Stopping: exceeded max_m")
            print(f"Converged at iteration {iteration}, Loss: {current_loss:.4f}")
            max_m_reached = True
            break

    # Update y_unlabeled
    y_unlabeled += alpha * d

    grad_norm = np.linalg.norm(gradients)

    #check for convergence
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}, Loss: {current_loss:.4f}")
        break

    if 'max_m_reached' in locals() and max_m_reached:
        break

    # Logging
    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}, Step Size: {alpha:.5f}, m: {m}")

import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.title("Loss Over Iterations (Gradient Descent with Armijo Rule)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""# **BCGD with GS RULE**"""

import numpy as np

max_iter = 500          # Number of iterations
# Stopping criterions
tolerance = 1e-4
min_improvement = 6

#hyperparameters
alpha_init = 1.0           # Initial step size
delta = 0.5                # Shrinkage factor (0 < delta < 1)
gamma = 0.05               # Sufficient decrease parameter (0 < gamma < 0.5)
max_m = 10                # Max shrinkage iterations per coordinate

# Initialize y_unlabeled with random values
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points

# Placeholder to track loss history
loss_history = []

for iteration in range(max_iter):
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
    loss_history.append(current_loss)

    # Compute full gradient vector
    full_gradients = np.array([
        gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled)
        for j in range(U)
    ])

    # Gauss-Southwell rule: pick the coordinate with the largest |gradient|
    j_star = np.argmax(np.abs(full_gradients))
    gradient = full_gradients[j_star]
    d = -gradient
    m = 0

    # Armijo rule
    while m < max_m:
        step = (delta ** m) * alpha_init
        y_trial = np.copy(y_unlabeled)
        y_trial[j_star] += step * d

        loss_trial = loss_function(W, W_hat, labels.flatten(), y_trial)

        if loss_trial <= current_loss + gamma * step * gradient * d:
            break
        m += 1

    # Update only the selected coordinate
    y_unlabeled[j_star] += (delta ** m) * alpha_init * d

    # Compute gradient norm for convergence check
    grad_norm = np.linalg.norm(full_gradients)

    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}")
        break

    if iteration > 3 and (loss_history[-3] - loss_history[-1]) < min_improvement:
        print(f"Converged at iteration {iteration}")
        break

    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")

# Plot loss curve
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.title("Loss Over Iterations (BCGD with Armijo + Gauss-Southwell Rule)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""# **COORDINATE MINIMIZATION**"""

#C - coordinate Way
import numpy as np

# Hyperparameters
alpha = 0.001  # Learning rate
max_iter = 500  # Number of iterations
tolerance = 1e-4  # Stopping criterion

# Initialize y_unlabeled with random values
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points

# Placeholder for the loss history to track convergence
loss_history = []

# Coordinate Minimization Loop
for iteration in range(max_iter):
    # Compute current loss
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
    loss_history.append(current_loss)

    # Loop over all coordinates (or parameters)
    for j in range(U):
        # Fix all coordinates except y_unlabeled[j]
        # Perform gradient descent to minimize the loss with respect to y_unlabeled[j]
        gradient = gradient_yj(j, W, W_hat, labels.flatten(), y_unlabeled)

        # Update the coordinate y_unlabeled[j] using gradient descent
        y_unlabeled[j] -= alpha * gradient  # Move towards the minimum

    # Check for convergence (if the change in y_unlabeled is very small)
    grad_norm = np.linalg.norm(gradient)  # Norm of the gradient
    if grad_norm < tolerance:
        print(f"Converged at iteration {iteration}")
        break

    # Print progress every 50 iterations
    if iteration % 50 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Gradient Norm: {grad_norm:.6f}")

# Optional: Plot the loss over iterations for convergence visualization
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.title("Loss Over Iterations (Coordinate Minimization)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

"""Coordinate Minimization GD, Alternative Implementation"""

#C - coordinate Way
import numpy as np

# Hyperparameters
alpha = 0.001  # Learning rate
max_iter = 500  # Number of iterations
tolerance = 1e-4  # Stopping criterion

# Initialize y_unlabeled with random values
y_unlabeled = np.random.rand(U)  # U is the number of unlabeled points

# Placeholder for the loss history to track convergence
loss_history = []
y_unlabeled_history = [y_unlabeled.copy()] # Track y_unlabeled for convergence check

print("Starting Coordinate Minimization (Alternative Update)...")

for iteration in range(max_iter):
    y_unlabeled_old = y_unlabeled.copy()

    # Compute current loss before the cycle of updates
    # The loss calculation is correct even with the large W_hat diagonal
    current_loss = loss_function(W, W_hat, labels.flatten(), y_unlabeled)
    loss_history.append(current_loss)

    # --- Cycle through each unlabeled coordinate (y_unlabeled[j]) ---
    for j in range(U):
        sum_W_hat_ju_except_j = np.sum(W_hat[j, :] * y_unlabeled) - W_hat[j, j] * y_unlabeled[j]
        sum_W_hat_j_except_j = np.sum(W_hat[j, :]) - W_hat[j, j]

        numerator = 2*np.sum(W[:, j] * labels.flatten()) + sum_W_hat_ju_except_j
        denominator = 2*np.sum(W[:, j]) + sum_W_hat_j_except_j

        if denominator > epsilon:
             y_unlabeled[j] = numerator / denominator

    # Calculate the maximum absolute change in y_unlabeled across this iteration's cycle
    max_change = np.max(np.abs(y_unlabeled - y_unlabeled_old))
    y_unlabeled_history.append(y_unlabeled.copy()) # Store updated y_unlabeled

    if max_change < tolerance:
        print(f"Converged at iteration {iteration} with max change {max_change:.6f}")
        break

    if iteration % 10 == 0:
        print(f"Iteration {iteration}, Loss: {current_loss:.4f}, Max Change: {max_change:.6f}")

if iteration == max_iter - 1:
    print(f"Maximum iterations ({max_iter}) reached.")

print("\nOptimization finished.")
print("Final y_unlabeled values:")
# print(y_unlabeled) # Too many values to print usually

y_pred = (y_unlabeled > 0.7).astype(int) # Changed threshold to 0.5
#print(y_unlabeled)
# Optional: Plot the loss over iterations for convergence visualization
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.title("Loss Over Iterations (Coordinate Minimization)")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.grid(True)
plt.show()
